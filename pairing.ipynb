{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/newtrainingdata/train/African' is a directory. Contents:\n",
      "['A4', 'A3', '.DS_Store', 'A2', 'A5', 'A19', 'A26', 'A21', 'A28', 'A17', 'A10', 'A11', 'A29', 'A16', 'A20', 'A18', 'A27', 'A30', 'A7', 'A9', 'A8', 'A6', 'A1', 'A22', 'A25', 'A13', 'A14', 'A15', 'A12', 'A24', 'A23']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "directory_path = \"/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/newtrainingdata/train/African\"\n",
    "\n",
    "if os.path.isdir(directory_path):\n",
    "    print(f\"'{directory_path}' is a directory. Contents:\")\n",
    "    contents = os.listdir(directory_path)\n",
    "    print(contents)\n",
    "else:\n",
    "    print(f\"'{directory_path}' is not a directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_paths(directory):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/MixFairFace-Image-Recognization-main/newtrainingdata/train/African'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdirs.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths_by_subdir = {subdir: get_image_paths(subdir) for subdir in subdirs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdir_idx, subdir in enumerate(subdirs):\n",
    "    image_paths = image_paths_by_subdir[subdir]\n",
    "    next_subdir_idx = (subdir_idx + 1) % len(subdirs)\n",
    "    next_subdir = subdirs[next_subdir_idx]\n",
    "    next_image_paths = image_paths_by_subdir[next_subdir]\n",
    "    \n",
    "    for image1, image2 in product(image_paths, next_image_paths):\n",
    "        image1_name = os.path.basename(image1)\n",
    "        image2_name = os.path.basename(image2)\n",
    "        subdir_name = os.path.basename(subdir)\n",
    "        pair_name = f\"{image1_name}*{image2_name} - {subdir_name}\"\n",
    "        print(pair_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "import csv\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "root_dir = '/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/MixFairFace-Image-Recognization-main/newtrainingdata/train/African'\n",
    "subdirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "image_paths_by_subdir = {subdir: get_image_paths(subdir) for subdir in subdirs}\n",
    "\n",
    "# Create a CSV file\n",
    "csv_file = 'image_pairs.csv'\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['id', 'image_1', 'image_2', 'folder'])\n",
    "\n",
    "    pair_id = 1\n",
    "    for subdir_idx, subdir in enumerate(subdirs):\n",
    "        image_paths = image_paths_by_subdir[subdir]\n",
    "        next_subdir_idx = (subdir_idx + 1) % len(subdirs)\n",
    "        next_subdir = subdirs[next_subdir_idx]\n",
    "        next_image_paths = image_paths_by_subdir[next_subdir]\n",
    "\n",
    "        for image1, image2 in product(image_paths, next_image_paths):\n",
    "            image1_name = os.path.basename(image1)\n",
    "            image2_name = os.path.basename(image2)\n",
    "            subdir_name = os.path.basename(subdir)\n",
    "            writer.writerow([pair_id, image1_name, image2_name, subdir_name])\n",
    "            pair_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "import csv\n",
    "\n",
    "def get_image_paths(directory):\n",
    "    image_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "    return image_paths\n",
    "\n",
    "root_dir = '/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/MixFairFace-Image-Recognization-main/newtrainingdata/train/African'\n",
    "subdirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "image_paths_by_subdir = {subdir: get_image_paths(subdir) for subdir in subdirs}\n",
    "\n",
    "# Create a CSV file\n",
    "csv_file = 'image_pairs_final.csv'\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['id', 'image_1', 'image_2', 'folder1', 'folder2'])\n",
    "\n",
    "    pair_id = 1\n",
    "    for subdir_idx, subdir in enumerate(subdirs):\n",
    "        image_paths = image_paths_by_subdir[subdir]\n",
    "        next_subdir_idx = (subdir_idx + 1) % len(subdirs)\n",
    "        next_subdir = subdirs[next_subdir_idx]\n",
    "        next_image_paths = image_paths_by_subdir[next_subdir]\n",
    "\n",
    "        for image1, image2 in product(image_paths, next_image_paths):\n",
    "            image1_name = os.path.basename(image1)\n",
    "            image2_name = os.path.basename(image2)\n",
    "            folder1_name = os.path.basename(subdir)\n",
    "            folder2_name = os.path.basename(next_subdir)\n",
    "            writer.writerow([pair_id, image1_name, image2_name, folder1_name, folder2_name])\n",
    "            pair_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "image_df = pd.read_csv(\"/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/MixFairFace-Image-Recognization-main/image_pairs_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>folder1</th>\n",
       "      <th>folder2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>2-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>1-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>7-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>19-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>74-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>4-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>16-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>6-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2-FaceId-0_align.jpg</td>\n",
       "      <td>2-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>2-FaceId-0_align.jpg</td>\n",
       "      <td>1-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                 image_1                image_2 folder1 folder2\n",
       "0   1  102-FaceId-0_align.jpg   2-FaceId-0_align.jpg      A4      A3\n",
       "1   2  102-FaceId-0_align.jpg   1-FaceId-0_align.jpg      A4      A3\n",
       "2   3  102-FaceId-0_align.jpg   7-FaceId-0_align.jpg      A4      A3\n",
       "3   4  102-FaceId-0_align.jpg  19-FaceId-0_align.jpg      A4      A3\n",
       "4   5  102-FaceId-0_align.jpg  74-FaceId-0_align.jpg      A4      A3\n",
       "5   6  102-FaceId-0_align.jpg   4-FaceId-0_align.jpg      A4      A3\n",
       "6   7  102-FaceId-0_align.jpg  16-FaceId-0_align.jpg      A4      A3\n",
       "7   8  102-FaceId-0_align.jpg   6-FaceId-0_align.jpg      A4      A3\n",
       "8   9    2-FaceId-0_align.jpg   2-FaceId-0_align.jpg      A4      A3\n",
       "9  10    2-FaceId-0_align.jpg   1-FaceId-0_align.jpg      A4      A3"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.2.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch==2.2.2->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch==2.2.2->torchvision) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch==2.2.2->torchvision) (1.3.0)\n",
      "Using cached torchvision-0.17.2-cp311-cp311-macosx_10_13_x86_64.whl (1.7 MB)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Identity()\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load the pre-trained ResNet model without the final fully connected layer\n",
    "resnet = models.resnet34(pretrained=False)\n",
    "resnet.fc = torch.nn.Identity()  # Replace the fully connected layer with an Identity layer\n",
    "\n",
    "# Load the weights from the .pth file\n",
    "checkpoint = torch.load('/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/resnet34_model.pth')\n",
    "resnet.load_state_dict(checkpoint, strict=False)  # Set strict=False to ignore missing keys\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "resnet.eval()\n",
    "\n",
    "# Now you can use this resnet model for feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "# Define preprocessing transforms\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "def extract_features(image_path, model):\n",
    "    image = Image.open(image_path)\n",
    "    image = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "    return features\n",
    "\n",
    "# Example usage:\n",
    "image_path = '/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/000005_00@.jpg'\n",
    "features = extract_features(image_path, resnet)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/MixFairFace-Image-Recognization-main/image_pairs_final.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Alternatively, you can use str.join() method\n",
    "df['File_path1'] = '/' + df['folder1'] + '/' + df['image_1']\n",
    "df['File_path2'] = '/' + df['folder2'] + '/' + df['image_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>image_1</th>\n",
       "      <th>image_2</th>\n",
       "      <th>folder1</th>\n",
       "      <th>folder2</th>\n",
       "      <th>File_path1</th>\n",
       "      <th>File_path2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>2-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "      <td>/A4/102-FaceId-0_align.jpg</td>\n",
       "      <td>/A3/2-FaceId-0_align.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>1-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "      <td>/A4/102-FaceId-0_align.jpg</td>\n",
       "      <td>/A3/1-FaceId-0_align.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>7-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "      <td>/A4/102-FaceId-0_align.jpg</td>\n",
       "      <td>/A3/7-FaceId-0_align.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>19-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "      <td>/A4/102-FaceId-0_align.jpg</td>\n",
       "      <td>/A3/19-FaceId-0_align.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>102-FaceId-0_align.jpg</td>\n",
       "      <td>74-FaceId-0_align.jpg</td>\n",
       "      <td>A4</td>\n",
       "      <td>A3</td>\n",
       "      <td>/A4/102-FaceId-0_align.jpg</td>\n",
       "      <td>/A3/74-FaceId-0_align.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71208</th>\n",
       "      <td>71209</td>\n",
       "      <td>43-FaceId-0_align.jpg</td>\n",
       "      <td>10-FaceId-0_align.jpg</td>\n",
       "      <td>A23</td>\n",
       "      <td>A4</td>\n",
       "      <td>/A23/43-FaceId-0_align.jpg</td>\n",
       "      <td>/A4/10-FaceId-0_align.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71209</th>\n",
       "      <td>71210</td>\n",
       "      <td>43-FaceId-0_align.jpg</td>\n",
       "      <td>98-FaceId-0_align.jpg</td>\n",
       "      <td>A23</td>\n",
       "      <td>A4</td>\n",
       "      <td>/A23/43-FaceId-0_align.jpg</td>\n",
       "      <td>/A4/98-FaceId-0_align.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71210</th>\n",
       "      <td>71211</td>\n",
       "      <td>43-FaceId-0_align.jpg</td>\n",
       "      <td>85-FaceId-0_align.jpg</td>\n",
       "      <td>A23</td>\n",
       "      <td>A4</td>\n",
       "      <td>/A23/43-FaceId-0_align.jpg</td>\n",
       "      <td>/A4/85-FaceId-0_align.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71211</th>\n",
       "      <td>71212</td>\n",
       "      <td>43-FaceId-0_align.jpg</td>\n",
       "      <td>88-FaceId-0_align.jpg</td>\n",
       "      <td>A23</td>\n",
       "      <td>A4</td>\n",
       "      <td>/A23/43-FaceId-0_align.jpg</td>\n",
       "      <td>/A4/88-FaceId-0_align.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71212</th>\n",
       "      <td>71213</td>\n",
       "      <td>43-FaceId-0_align.jpg</td>\n",
       "      <td>43-FaceId-0_align.jpg</td>\n",
       "      <td>A23</td>\n",
       "      <td>A4</td>\n",
       "      <td>/A23/43-FaceId-0_align.jpg</td>\n",
       "      <td>/A4/43-FaceId-0_align.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71213 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                 image_1                image_2 folder1 folder2  \\\n",
       "0          1  102-FaceId-0_align.jpg   2-FaceId-0_align.jpg      A4      A3   \n",
       "1          2  102-FaceId-0_align.jpg   1-FaceId-0_align.jpg      A4      A3   \n",
       "2          3  102-FaceId-0_align.jpg   7-FaceId-0_align.jpg      A4      A3   \n",
       "3          4  102-FaceId-0_align.jpg  19-FaceId-0_align.jpg      A4      A3   \n",
       "4          5  102-FaceId-0_align.jpg  74-FaceId-0_align.jpg      A4      A3   \n",
       "...      ...                     ...                    ...     ...     ...   \n",
       "71208  71209   43-FaceId-0_align.jpg  10-FaceId-0_align.jpg     A23      A4   \n",
       "71209  71210   43-FaceId-0_align.jpg  98-FaceId-0_align.jpg     A23      A4   \n",
       "71210  71211   43-FaceId-0_align.jpg  85-FaceId-0_align.jpg     A23      A4   \n",
       "71211  71212   43-FaceId-0_align.jpg  88-FaceId-0_align.jpg     A23      A4   \n",
       "71212  71213   43-FaceId-0_align.jpg  43-FaceId-0_align.jpg     A23      A4   \n",
       "\n",
       "                       File_path1                 File_path2  \n",
       "0      /A4/102-FaceId-0_align.jpg   /A3/2-FaceId-0_align.jpg  \n",
       "1      /A4/102-FaceId-0_align.jpg   /A3/1-FaceId-0_align.jpg  \n",
       "2      /A4/102-FaceId-0_align.jpg   /A3/7-FaceId-0_align.jpg  \n",
       "3      /A4/102-FaceId-0_align.jpg  /A3/19-FaceId-0_align.jpg  \n",
       "4      /A4/102-FaceId-0_align.jpg  /A3/74-FaceId-0_align.jpg  \n",
       "...                           ...                        ...  \n",
       "71208  /A23/43-FaceId-0_align.jpg  /A4/10-FaceId-0_align.jpg  \n",
       "71209  /A23/43-FaceId-0_align.jpg  /A4/98-FaceId-0_align.jpg  \n",
       "71210  /A23/43-FaceId-0_align.jpg  /A4/85-FaceId-0_align.jpg  \n",
       "71211  /A23/43-FaceId-0_align.jpg  /A4/88-FaceId-0_align.jpg  \n",
       "71212  /A23/43-FaceId-0_align.jpg  /A4/43-FaceId-0_align.jpg  \n",
       "\n",
       "[71213 rows x 7 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Fi\"] = (\"/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/MixFairFace-Image-Recognization-main/newtrainingdata/train/African\" + df[\"File_path1\"]).apply(extract_features, args=(resnet,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/Users/deepak/Desktop/study material/mtech:sem1/sem 2/AML/Fairness-Image-Recognization/MixFairFace-Image-Recognization-main/image_pair_African_feature.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
